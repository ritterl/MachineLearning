{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter10ML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOU0EJqsSy3n0fIWd44okvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritterl/MachineLearning/blob/master/Chapter10ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pstxqq-xHwr6",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WCzthDOqLQz",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, chapter 10 of [Machine Learning](https://www.amazon.de/Machine-Learning-techniques-predictive-modeling-ebook/dp/B07PYXX3H5) with R is summarized and the code samples are described. This section of the book serves to show how machine learning algorithms can be evaluated. More precisely, it gives reasons why also other measures than predictive accuracy are needed to assess performance. Further, approaches to ensure that the performance measures reasonably reflect a model's ability to predict or forecast unseen cases are provided. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXmPjZa4ichB",
        "colab_type": "text"
      },
      "source": [
        "#Measuring performance for classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QsUZ7XTyJp0",
        "colab_type": "text"
      },
      "source": [
        "By simply dividing the number of correct predictions by the total number of predictions, a wrong picture about the performance of the classifier may occur. This especially arises in datasets with a large class imbalance and is also referred to as **class imbalance problem**. For instance, if a positive event occurs very often (in say 99% of the cases), a model which predicts always a positive case has an accuracy of 99%. However, it is not useful for predicting the negative cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb0llg1qiuBO",
        "colab_type": "text"
      },
      "source": [
        "# Understanding a classifier's predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oVcK8pQMxpJ",
        "colab_type": "text"
      },
      "source": [
        "Besides the two important data types: **actual class values** and **predicted class values** which are obvious, however, the majority of models can deliver another important type of information: the **estimated probability of the prediction** or in other words the confidence of the model about a particular decision. So, when comparing two models with the same number of mistakes, it is possible to say that the one which makes better assessments regarding its uncertainty is smarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkLp-x-fjC42",
        "colab_type": "text"
      },
      "source": [
        "First of all, the predicted probabilities from the SMS model developed in chapter 4 of the book are drawn. Therefore, it is important to note that the code of chapter 4 needs to be executed prior to this section. In this example, the `predict() `function gives the probability for each possible outcome. Each line of the following output sums up to 1 due to the fact that these are mutually exclusive and exhaustive events. In other words: An SMS can only either be \"ham\" or \"spam\", but not both at the same time and it cannot be something else or something in between. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge6rq4Jvn6SS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsiYmbmr19ZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "649f6601-3162-402b-ec5b-168c76683318"
      },
      "source": [
        "\n",
        "sms_classifier <- load(\"https://github.com/ritterl/MachineLearning/blob/master/sms_classifier.RData?raw=true\")\n",
        "install.packages(\"tm\")\n",
        "library(tm)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in readChar(con, 5L, useBytes = TRUE):\n",
            "“cannot open compressed file 'https://github.com/ritterl/MachineLearning/blob/master/sms_classifier.RData?raw=true', probable reason 'No such file or directory'”\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "ignored",
          "traceback": [
            "Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection\nTraceback:\n",
            "1. load(\"https://github.com/ritterl/MachineLearning/blob/master/sms_classifier.RData?raw=true\")",
            "2. readChar(con, 5L, useBytes = TRUE)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dLLy_C1zcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages(\"caret\")\n",
        "library(caret)\n",
        "# obtain the predicted probabilities\n",
        "sms_test_prob <- predict(sms_classifier, sms_test, type = \"raw\")\n",
        "head(sms_test_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyDcxbFy5_r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Confusion matrices in R ----\n",
        "sms_results <- read.csv(\"https://raw.githubusercontent.com/ritterl/MachineLearning/master/sms_results.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMLR4-B-6Z8A",
        "colab_type": "text"
      },
      "source": [
        "In this step a first glimpse into the sms_results is made. It shows the actual type as well as the predicted type on the LHS, and on the RHS the probability (estimated by the model) of the object being either spam or ham. As it can be observed, the model was extremely certain about its decisions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_z3MFw1h4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0a205361-eb4d-41d8-fbe2-5fa913a3957d"
      },
      "source": [
        "# the first several test cases\n",
        "head(sms_results)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  actual_type predict_type prob_spam prob_ham\n",
              "1 ham         ham          0.00000   1.00000 \n",
              "2 ham         ham          0.00000   1.00000 \n",
              "3 ham         ham          0.00016   0.99984 \n",
              "4 ham         ham          0.00004   0.99996 \n",
              "5 spam        spam         1.00000   0.00000 \n",
              "6 ham         ham          0.00020   0.99980 "
            ],
            "text/latex": "A data.frame: 6 × 4\n\\begin{tabular}{r|llll}\n  & actual\\_type & predict\\_type & prob\\_spam & prob\\_ham\\\\\n  & <fct> & <fct> & <dbl> & <dbl>\\\\\n\\hline\n\t1 & ham  & ham  & 0.00000 & 1.00000\\\\\n\t2 & ham  & ham  & 0.00000 & 1.00000\\\\\n\t3 & ham  & ham  & 0.00016 & 0.99984\\\\\n\t4 & ham  & ham  & 0.00004 & 0.99996\\\\\n\t5 & spam & spam & 1.00000 & 0.00000\\\\\n\t6 & ham  & ham  & 0.00020 & 0.99980\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 6 × 4\n\n| <!--/--> | actual_type &lt;fct&gt; | predict_type &lt;fct&gt; | prob_spam &lt;dbl&gt; | prob_ham &lt;dbl&gt; |\n|---|---|---|---|---|\n| 1 | ham  | ham  | 0.00000 | 1.00000 |\n| 2 | ham  | ham  | 0.00000 | 1.00000 |\n| 3 | ham  | ham  | 0.00016 | 0.99984 |\n| 4 | ham  | ham  | 0.00004 | 0.99996 |\n| 5 | spam | spam | 1.00000 | 0.00000 |\n| 6 | ham  | ham  | 0.00020 | 0.99980 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 6 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>actual_type</th><th scope=col>predict_type</th><th scope=col>prob_spam</th><th scope=col>prob_ham</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>ham </td><td>ham </td><td>0.00000</td><td>1.00000</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>ham </td><td>ham </td><td>0.00000</td><td>1.00000</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>ham </td><td>ham </td><td>0.00016</td><td>0.99984</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>ham </td><td>ham </td><td>0.00004</td><td>0.99996</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>spam</td><td>spam</td><td>1.00000</td><td>0.00000</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>ham </td><td>ham </td><td>0.00020</td><td>0.99980</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS4f7LPw7D-d",
        "colab_type": "text"
      },
      "source": [
        "In the previous cases, the model was very confident, but of course, there are also other cases in which the model was unconfident about its decision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3RI3Er8BSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "40f3ffcb-6aa0-4c27-bde4-a9471f2405a2"
      },
      "source": [
        "head(subset(sms_results, prob_spam > 0.40 & prob_spam < 0.60))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     actual_type predict_type prob_spam prob_ham\n",
              "377  spam        ham          0.47536   0.52464 \n",
              "717  ham         spam         0.56188   0.43812 \n",
              "1311 ham         spam         0.57917   0.42083 "
            ],
            "text/latex": "A data.frame: 3 × 4\n\\begin{tabular}{r|llll}\n  & actual\\_type & predict\\_type & prob\\_spam & prob\\_ham\\\\\n  & <fct> & <fct> & <dbl> & <dbl>\\\\\n\\hline\n\t377 & spam & ham  & 0.47536 & 0.52464\\\\\n\t717 & ham  & spam & 0.56188 & 0.43812\\\\\n\t1311 & ham  & spam & 0.57917 & 0.42083\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 3 × 4\n\n| <!--/--> | actual_type &lt;fct&gt; | predict_type &lt;fct&gt; | prob_spam &lt;dbl&gt; | prob_ham &lt;dbl&gt; |\n|---|---|---|---|---|\n| 377 | spam | ham  | 0.47536 | 0.52464 |\n| 717 | ham  | spam | 0.56188 | 0.43812 |\n| 1311 | ham  | spam | 0.57917 | 0.42083 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 3 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>actual_type</th><th scope=col>predict_type</th><th scope=col>prob_spam</th><th scope=col>prob_ham</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>377</th><td>spam</td><td>ham </td><td>0.47536</td><td>0.52464</td></tr>\n",
              "\t<tr><th scope=row>717</th><td>ham </td><td>spam</td><td>0.56188</td><td>0.43812</td></tr>\n",
              "\t<tr><th scope=row>1311</th><td>ham </td><td>spam</td><td>0.57917</td><td>0.42083</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNTw5YZH8WFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ee3e637d-46c3-4911-fd58-530bacd93aae"
      },
      "source": [
        "head(subset(sms_results, actual_type != predict_type))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    actual_type predict_type prob_spam prob_ham\n",
              "53  spam        ham          0.00071   0.99929 \n",
              "59  spam        ham          0.00156   0.99844 \n",
              "73  spam        ham          0.01708   0.98292 \n",
              "76  spam        ham          0.00851   0.99149 \n",
              "184 spam        ham          0.01243   0.98757 \n",
              "332 spam        ham          0.00003   0.99997 "
            ],
            "text/latex": "A data.frame: 6 × 4\n\\begin{tabular}{r|llll}\n  & actual\\_type & predict\\_type & prob\\_spam & prob\\_ham\\\\\n  & <fct> & <fct> & <dbl> & <dbl>\\\\\n\\hline\n\t53 & spam & ham & 0.00071 & 0.99929\\\\\n\t59 & spam & ham & 0.00156 & 0.99844\\\\\n\t73 & spam & ham & 0.01708 & 0.98292\\\\\n\t76 & spam & ham & 0.00851 & 0.99149\\\\\n\t184 & spam & ham & 0.01243 & 0.98757\\\\\n\t332 & spam & ham & 0.00003 & 0.99997\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 6 × 4\n\n| <!--/--> | actual_type &lt;fct&gt; | predict_type &lt;fct&gt; | prob_spam &lt;dbl&gt; | prob_ham &lt;dbl&gt; |\n|---|---|---|---|---|\n| 53 | spam | ham | 0.00071 | 0.99929 |\n| 59 | spam | ham | 0.00156 | 0.99844 |\n| 73 | spam | ham | 0.01708 | 0.98292 |\n| 76 | spam | ham | 0.00851 | 0.99149 |\n| 184 | spam | ham | 0.01243 | 0.98757 |\n| 332 | spam | ham | 0.00003 | 0.99997 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 6 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>actual_type</th><th scope=col>predict_type</th><th scope=col>prob_spam</th><th scope=col>prob_ham</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>53</th><td>spam</td><td>ham</td><td>0.00071</td><td>0.99929</td></tr>\n",
              "\t<tr><th scope=row>59</th><td>spam</td><td>ham</td><td>0.00156</td><td>0.99844</td></tr>\n",
              "\t<tr><th scope=row>73</th><td>spam</td><td>ham</td><td>0.01708</td><td>0.98292</td></tr>\n",
              "\t<tr><th scope=row>76</th><td>spam</td><td>ham</td><td>0.00851</td><td>0.99149</td></tr>\n",
              "\t<tr><th scope=row>184</th><td>spam</td><td>ham</td><td>0.01243</td><td>0.98757</td></tr>\n",
              "\t<tr><th scope=row>332</th><td>spam</td><td>ham</td><td>0.00003</td><td>0.99997</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqwfFpvv8hor",
        "colab_type": "text"
      },
      "source": [
        "Especially in these cases the question can be posed, whether the model is useful or not. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGhuuOaj7ASz",
        "colab_type": "text"
      },
      "source": [
        "# A closer look at confusion matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD6Dz6QhUp73",
        "colab_type": "text"
      },
      "source": [
        "A **confusion** matrix has two dimensions. One dimension displays the actual values (rows) and the other dimension displays the predicted values (columns). In the diagonal cells the model predicted the actual value correct and in the off-diagonal cells the predictions were incorrect. In R, a confusion matrix can be created with the `table()` function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPBevmrmUqMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "463a5b8d-0845-4b3b-e257-21f28a31ec4e"
      },
      "source": [
        "table(sms_results$actual_type, sms_results$predict_type)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      \n",
              "        ham spam\n",
              "  ham  1203    4\n",
              "  spam   31  152"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms1X6hsAX6II",
        "colab_type": "text"
      },
      "source": [
        "With the introduction of confusion matrices, also a new terminology needs to be implemented, whereas first of all the class of interest needs to be defined. In the ham/spam example, \"spam\" is referred to as the **positive class**, since the spam-filter is interested in finding spam messages. Consequently, the **negative class** are the ham messages. It should be obvious that positive and negative are not related with \"good and bad\", but solely needed to distinguish between the different classes. \n",
        "However, now it is possible to implement this terminology in the confusion matrix. \n",
        "- predicted spam / actual spam = True positive (TP -> 152)\n",
        "- predicted ham / acutal ham = True negative (TN -> 1203)\n",
        "- predicted spam / actual ham = False positive (FP -> 4)\n",
        "- predicted ham / actual spam = False negative (FN -> 31)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdgbW19iWvR",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrices are a helpful tool to measure performance. For instance, the accuracy can easily be calculated as following: accuracy = (TP + TN)/(TP + TN + FP + FN). Also the error rate is very simple to calculate: error rate = (FP + FN)/(TP + TN + FP + FN) = 1 - accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFQx_hKYlLwq",
        "colab_type": "text"
      },
      "source": [
        "Besides the previous confusion matrix which contains no additional information, it is also possible to create a matrix with more informative value using the `CrossTable()` function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYtbgZhYliL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "3e13cd15-91b5-4581-c281-b2053062a978"
      },
      "source": [
        "# using the CrossTable function\n",
        "install.packages(\"gmodels\")\n",
        "library(gmodels)\n",
        "CrossTable(sms_results$actual_type, sms_results$predict_type)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘gtools’, ‘gdata’\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            "   Cell Contents\n",
            "|-------------------------|\n",
            "|                       N |\n",
            "| Chi-square contribution |\n",
            "|           N / Row Total |\n",
            "|           N / Col Total |\n",
            "|         N / Table Total |\n",
            "|-------------------------|\n",
            "\n",
            " \n",
            "Total Observations in Table:  1390 \n",
            "\n",
            " \n",
            "                        | sms_results$predict_type \n",
            "sms_results$actual_type |       ham |      spam | Row Total | \n",
            "------------------------|-----------|-----------|-----------|\n",
            "                    ham |      1203 |         4 |      1207 | \n",
            "                        |    16.128 |   127.580 |           | \n",
            "                        |     0.997 |     0.003 |     0.868 | \n",
            "                        |     0.975 |     0.026 |           | \n",
            "                        |     0.865 |     0.003 |           | \n",
            "------------------------|-----------|-----------|-----------|\n",
            "                   spam |        31 |       152 |       183 | \n",
            "                        |   106.377 |   841.470 |           | \n",
            "                        |     0.169 |     0.831 |     0.132 | \n",
            "                        |     0.025 |     0.974 |           | \n",
            "                        |     0.022 |     0.109 |           | \n",
            "------------------------|-----------|-----------|-----------|\n",
            "           Column Total |      1234 |       156 |      1390 | \n",
            "                        |     0.888 |     0.112 |           | \n",
            "------------------------|-----------|-----------|-----------|\n",
            "\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHKLsC55mgqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy and error rate calculation --\n",
        "# accuracy\n",
        "(152 + 1203) / (152 + 1203 + 4 + 31)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LToQdMHAmy5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# error rate\n",
        "(4 + 31) / (152 + 1203 + 4 + 31)\n",
        "# error rate = 1 - accuracy\n",
        "1 - 0.9748201"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9nkzWFF7kqB",
        "colab_type": "text"
      },
      "source": [
        "# Beyond accuracy - other measures of performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82DKA72n_QL",
        "colab_type": "text"
      },
      "source": [
        "In the following section alternative performance measures will be shown. For this purpose, the `caret` package developed by Max Kuhn is required. The syntax is close to the `table()` function, but with a slight distinction. It is required to define the positive class (as described above), since `caret` evaluates performance based on the ability to classify the positive (spam) class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q91mOtu_8C3R",
        "colab_type": "text"
      },
      "source": [
        "# The kappa statistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lmciVMVr2G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages(\"e1071\")\n",
        "library(e1071)\n",
        "confusionMatrix(sms_results$predict_type, sms_results$actual_type, positive = \"spam\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejqzGCIbuBX8",
        "colab_type": "text"
      },
      "source": [
        "Especially the **Kappa statistic** (range from 0 to 1) is interesting in this output, since it adjusts the accuracy of a model by taking into account correct predictions by chance. When taking the introductory example with the class imbalance problem and a model which always predicts \"positive\", the Kappa statistic would reveal the weakness of this model. Hence, only classifiers which are more often correct than \"correct predictions by chance\" are rewarded by Kappa. The formula for kappa is the following: \n",
        "(Pr(a) - Pr(e))/(1-Pr(e)), where Pr(a) represents the proportion of actual agreement and Pr(e) represents the expected agreement (between the classifier and the true values)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S29AR1W3SO7t",
        "colab_type": "text"
      },
      "source": [
        "In order to compute the observed agreement simply the proportion of the TP's and TN's must be summed up, which is again the already known measure \"accuracy\". The data can be taken from the `CrossTable()` above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5rCmKL-SNsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example using SMS classifier\n",
        "pr_a <- 0.865 + 0.109\n",
        "pr_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oQnhovMT7xz",
        "colab_type": "text"
      },
      "source": [
        "The calculation for Pr(e) is a bit more complicated. First of all, it is noteworthy that the following calculation holds only under the assumption that the events are **independent** from each other. Following this assumption, it is possible to apply a probability rule which says that the probability of both occuring, is nothing else than the product of the probabilities.\n",
        "- Pr(actual_type is ham)*Pr(predicted_type is ham)\n",
        "-Pr(actual_type is spam)*Pr(predicted_type is spam). Again, these probabilites can be found in the `CrossTable()`.\n",
        "Finally, Pr(e) is the sum of the probabilities that the predicted and actual values agree. This is possible because the two events are mutually exlcusive (a SMS can either be ham or spam but there is no intersection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3vzlM-BWZOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_e <- 0.868 * 0.888 + 0.132 * 0.112\n",
        "pr_e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9DKTB3MXIvE",
        "colab_type": "text"
      },
      "source": [
        "Applying the pre-defined formular for Kappa leads to: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb3g9eIoXNtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k <- (pr_a - pr_e) / (1 - pr_e)\n",
        "k\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIkPJY08Xptm",
        "colab_type": "text"
      },
      "source": [
        "which is in line with the result generated by the `confusionMatrix()` before. However, there are also other possibilities to obtain R which will not be explained in detail at this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avEeExw3YPhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate kappa via the vcd package\n",
        "install.packages(\"vcd\")\n",
        "library(vcd)\n",
        "Kappa(table(sms_results$actual_type, sms_results$predict_type))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp3DNuJDYcQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate kappa via the irr package\n",
        "install.packages(\"irr\")\n",
        "library(irr)\n",
        "kappa2(sms_results[1:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiqidmoi8bid",
        "colab_type": "text"
      },
      "source": [
        "# Sensitivity and specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrwtoh0xa267",
        "colab_type": "text"
      },
      "source": [
        "In the next section, two very important measures will be introduced: sensitivity and specificity. They are important to find a balanced model which is neither too conservative nor too aggressive. When taking the SMS example: On the one hand, this means that the model should not classify almost every single SMS (specificity) as spam and on the other hand, it should not allow too many spam SMS getting through the filter (sensitivity). It is apparent that these two goals can be considered a tradeoff. Therefore, it is important to decide, which measure is more relevant. With the SMS example, it can be said that it is worse if a ham SMS is deleted because it is considered as spam, than if a spam SMS is not deleted and it ends up in the inbox. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-KBQLhem4Dr",
        "colab_type": "text"
      },
      "source": [
        "The sensitivity is also referred to as true positive rate and is calculated as following: sensitivity = TP / (TP + FN). Using the numbers of the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN2-Yg7jpHTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sens <- 152 / (152 + 31)\n",
        "sens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnJWsbVcpiV1",
        "colab_type": "text"
      },
      "source": [
        "In contrast, the specificity is also referred to as true negative rate and is calculated as following: specificity = TN / (TN + FP). Using the numbers of the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-mZwER6qaZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spec <- 1203 / (1203 + 4)\n",
        "spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ee5Tgrxqp7t",
        "colab_type": "text"
      },
      "source": [
        "It is also possible to calculate the measures directly with the aid of the `caret` package. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72i8LFIGrXYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example using the caret package\n",
        "install.packages(\"caret\")\n",
        "library(caret)\n",
        "sensitivity(sms_results$predict_type, sms_results$actual_type, positive = \"spam\")\n",
        "specificity(sms_results$predict_type, sms_results$actual_type, negative = \"ham\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iIQyJBX9f7y",
        "colab_type": "text"
      },
      "source": [
        "# Precision and recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIsgQCrmsf62",
        "colab_type": "text"
      },
      "source": [
        "In the following section two new measures will be introduced: precision and recall. When looking at their formulaes, the similarity with specificity and sensitivity become quickly apparent. However, there are slight differences. **Precision** can be regarded as a measure that displays how often the model is correct when predicting the **positive** class. Therefore, a precise model predicts the positive class only, if it is very confident about its decision. When applying the SMS filter, a high precision would mean that the model is only filtering and deleting messages that are actually spam. The formula is the following: precision = TP / (TP + FP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-65dmRI0kmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using the numbers from the confusion matrix\n",
        "prec <- 152 / (152 + 4)\n",
        "prec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9D5MFxQ21z8",
        "colab_type": "text"
      },
      "source": [
        "This number says that if the model predicts spam, it is in almost 97.5% of the cases correct. \n",
        "The other measure evaluates the completeness of the results. The formula is the same as for sensitivity: recall = TP / (TP + FN), but the interpretation is different. If the SMS model has a high recall, a large fraction of the spam messages are correctly determined. Again, recall can easily be computed by using the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpYyoLyn4Gzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rec <- 152 / (152 + 31)\n",
        "rec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgW3GYoC4Qdk",
        "colab_type": "text"
      },
      "source": [
        "Also by using the `caret ` package, precision and recall can be calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI6IGBKb4e6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example using the caret package\n",
        "posPredValue(sms_results$predict_type, sms_results$actual_type, positive = \"spam\")\n",
        "sensitivity(sms_results$predict_type, sms_results$actual_type, positive = \"spam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQUlmN-p-Mb3",
        "colab_type": "text"
      },
      "source": [
        "# The F-measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kozjfwu5LdR",
        "colab_type": "text"
      },
      "source": [
        "A further interesting figure is the so-called **F-measure**. The F-measure has the nice feature that one single number subsumizes the previously introduced concepts precision and recall and therefore, allows a convenient comparison of models. In the F-measure, precision and recall are combined in a particular way of average which is utilized for rates of change and called **harmonic mean**: The formula is the following: F-measure = (2x prec * rec) / (rec + prec)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIx7FQUlM7ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# F-measure\n",
        "f <- (2 * prec * rec) / (prec + rec)\n",
        "f\n",
        "\n",
        "# Alternative calculation by using numbers from confusion matrix\n",
        "f <- (2 * 152) / (2 * 152 + 4 + 31)\n",
        "f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn7js1Cw-dzX",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing performance tradeoffs with ROC curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kelnN2O1PONc",
        "colab_type": "text"
      },
      "source": [
        "So far, only \"numbers\" have been introduced, but it is also possible to visualize performance by aid of **ROC curves** (receiver operating characteristic curve) for instance. Different models have different strenghts and weaknesses and even if they have the same accuracy it is possible that their decisions and their confidence varies greatly. Typically, the y-axis of the ROC curve represents the TP-rate (sensitivity) and the x-axis represents the FP-rate (= 1 - specificity). Hence, by using ROC curves it is possible to get a deeper understanding of tradeoffs between models with distinct characteristics. To create a ROC curve, the `pRoc` package is needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRJXyKUYTZY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Visualizing Performance Tradeoffs ----\n",
        "install.packages(\"pROC\")\n",
        "library(pROC)\n",
        "sms_roc <- roc(sms_results$actual_type, sms_results$prob_spam)\n",
        "# ROC curve for Naive Bayes\n",
        "plot(sms_roc, main = \"ROC curve for SMS spam filter\", col = \"blue\", lwd = 2, legacy.axes = TRUE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfuMr1-Shdj8",
        "colab_type": "text"
      },
      "source": [
        "In this plot a ROC curve for the SMS example is shown. The diagonal grey line represents a model that only predicts by \"chance\" and therefore, has no predictive value, since it predicts TP's and FP's at the same rate. On the contrary, a model with perfect predictive value would go through the 100% TP-rate. The blue line depicts the SMS spam filter model, which obviously has a very high predictive value. Simply put, the larger the area between the blue line and the grey line, the better the classifier. It is possible to calculate the area under the ROC curve **(AUC)**, whereas 0.5 represents a model with no predictive power and 1 a model with perfect predictive power. As the AUC can be the same for differently shaped curves, it is important not to use solely the AUC, but also other measures.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVzUm4J6l1s_",
        "colab_type": "text"
      },
      "source": [
        "It is also feasible to compare different models, for instance the k-NN model developed in chapter 3 with the Naive Bayes SMS classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlWnKIXdTTKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages(\"caret\")\n",
        "install.packages(\"e1071\")\n",
        "library(caret)\n",
        "library(e1071)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wwM24pEmdH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare to kNN \n",
        "sms_results_knn <- read.csv(\"https://raw.githubusercontent.com/ritterl/MachineLearning/master/sms_results_knn.csv\") \n",
        "sms_roc_knn <- roc(sms_results$actual_type, sms_results_knn$p_spam)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flber2WzU6Wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot(sms_roc_knn, col = \"red\", lwd = 2, add = TRUE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-syJweXBUSbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate AUC for Naive Bayes and kNN\n",
        "auc(sms_roc)\n",
        "auc(sms_roc_knn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t423IdWtqlP3",
        "colab_type": "text"
      },
      "source": [
        "As the area under the red line is obviously smaller, the k-NN approach is less powerful than the Naive Bayes model. This is also confirmed by the AUC.\n",
        "However, it is not proven if the Naive Bayes model is still better if it is applied to another dataset. Hence, it is important to be able to estimate future performance, which will be the topic of the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBb1igMMAx5F",
        "colab_type": "text"
      },
      "source": [
        "# Estimating future performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04IC2dN2vZnh",
        "colab_type": "text"
      },
      "source": [
        "In order to reveal very poor machine learners, a number of packages in R already show performance measures during the model-building process, which gives insight about the **resubstitution error**. It occurs if the model cannot predict the data correctly, which were used to build and train the model. Nevertheless, it is not possible to say whether a model with a low resubstitution error performs well when applying it to new data. To circumvent this problem, the whole dataset was separated randomly into a training and a test dataset **(holdout method)**, whereas the model is built upon the training dataset and evaluated with the test dataset.\n",
        "In an improved version of the holdout method, the dataset is split into a training, a validation and a test dataset in order to overcome biased results, which stem from cherry-picking the best model based on repeated testing. The introduction of a validation dataset can be seen as an intermediate step which helps to improve the model without implementing biased results. In the following code chunk, the different datasets are randomly generated with the `runif() `function. By using `the order()` function, a vector can be created that displays the rank order of the 1000 numbers. Example: `order(c(0.2, 0.4, 0.3))` would result in: `1 3 2`. Because the smallest number (0.2) appears on the first place, the second smallest number (0.3) appears on the third place and the largest number (0.4) appears on the second place.\n",
        "In the next step, the random ID's are separated into the pre-defined datasets (50% training, 25% validate and 25% test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpRu8B3iEuvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# partitioning data\n",
        "library(caret)\n",
        "credit <- read.csv(\"https://raw.githubusercontent.com/ritterl/MachineLearning/master/credit.csv\")\n",
        "# Holdout method\n",
        "# using random IDs\n",
        "random_ids <- order(runif(1000))\n",
        "credit_train <- credit[random_ids[1:500],]\n",
        "credit_validate <- credit[random_ids[501:750], ]\n",
        "credit_test <- credit[random_ids[751:1000], ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CFjFjh4FW94",
        "colab_type": "text"
      },
      "source": [
        "This is a nice approach, but it incurs a disadvantage. If there is a large class imbalance in the dataset, it could occur that the randomly defined training dataset cannot learn a particular class. An method to reduce this possibility is the so-called **stratified random sampling** where the subsets consist of the same proportions of classes as the original dataset. With the `createDataPartition()` function from the `caret` package, this can be achieved. \n",
        "**Maybe write more here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo2haVWTFJFs",
        "colab_type": "text"
      },
      "source": [
        "But in cases with small datasets, it does not make sense to split it up further. \n",
        "In this situations, the `caret` package provides alternative paths to estimate future performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps8lP5zZBdkr",
        "colab_type": "text"
      },
      "source": [
        "# Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFifSjWcdR6w",
        "colab_type": "text"
      },
      "source": [
        "In this section, the concept behind cross-validation will be introduced. When using the k-fold cross validation, the original dataset is split up into usually 10 random partitions (folds) wheras each fold represents 10% of the data. Thus, the model is based on 90% of the data and finally evaluated with 10% of the data. So, each single fold serves once as the test dataset and 9 times it is used in the training dataset. After repeating this process 10 times, the average performance over all 10 folds will be taken. With the `createFolds()` function from the `caret` package, the folds will randomly be created under consideration of the class balance of the original dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-358j4FCgKWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10-fold CV\n",
        "install.packages(\"caret\")\n",
        "library(caret)\n",
        "folds <- createFolds(credit$default, k = 10)\n",
        "str(folds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgmHx-BUrQsa",
        "colab_type": "text"
      },
      "source": [
        "So, in the first fold 100 random numbers between 1 and 1000 are contained. Each of these numbers represents a row of the original dataset. However, a further step is needed in order to create the model. The chosen 10% are assigned to the test dataset and the remainder (90%) is assigned to the training dataset using the negative symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg90BlAjon_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit01_test <- credit[folds$Fold01, ]\n",
        "credit01_train <- credit[-folds$Fold01, ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzX7R2EdtWPt",
        "colab_type": "text"
      },
      "source": [
        "Instead of doing this 10 times, it is possible to automate this step, where different packages are needed: `caret` (for folds), `C50` (for decision tree) and `irr` for kappa. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RGku7xGtzYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Automating 10-fold CV for a C5.0 Decision Tree using lapply() ----\n",
        "install.packages(\"C50\")\n",
        "library(caret)\n",
        "library(C50)\n",
        "library(irr)\n",
        "\n",
        "credit <- read.csv(\"https://raw.githubusercontent.com/ritterl/MachineLearning/master/credit.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWQKVmhrv0Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNGversion(\"3.5.2\") # use an older random number generator to match the book\n",
        "set.seed(123)\n",
        "folds <- createFolds(credit$default, k = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7GBND1swJuR",
        "colab_type": "text"
      },
      "source": [
        "In the next chunk, the `lapply()` function is \"feeded\" with a customized function that carries out what is needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvfEsU_wb-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_results <- lapply(folds, function(x) {\n",
        "  credit_train <- credit[-x, ]\n",
        "  credit_test <- credit[x, ]\n",
        "  credit_model <- C5.0(default ~ ., data = credit_train)\n",
        "  credit_pred <- predict(credit_model, credit_test)\n",
        "  credit_actual <- credit_test$default\n",
        "  kappa <- kappa2(data.frame(credit_actual, credit_pred))$value\n",
        "  return(kappa)\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LYo-RggxWID",
        "colab_type": "text"
      },
      "source": [
        "The lines above deliver 10 kappa statistics, for each of the folds 1. So, in a final step the average kappa can be calculated. However, in order to do so, the results have to be \"unlisted\", otherwise an error would occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyR2KnJ2yAzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(cv_results)\n",
        "mean(unlist(cv_results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa88ZYEBBi52",
        "colab_type": "text"
      },
      "source": [
        "# Bootstrap sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqf71bARybmJ",
        "colab_type": "text"
      },
      "source": [
        "In the last paragraph of this summary, the concept of bootsrap sampling will be briefly introduced. \n",
        "Bootstrapping is a statistical method where random samples are used to estimate properties of a larger set. In machine learning, bootstrapping can be used to create various random test and training datasets to estimate model performance. In contrast to k-fold CV, **sampling with replacement** is possible whereas in CV every single example is used only once. However, the average bootstrap training dataset contains only 63.2% of data, compared to the 90% of the 10-fold CV. So, it is highly likely that the bootstrapping method generates a worse model. Nevertheless, it is possible to account for this drawback by computing the performance as a function on the training data (overly optimistic) and the test data (pessimistic).\n",
        "One positive side of bootstrapping is that it might work better with very small datasets, compared to k-fold CV. Further, bootstrapping can also be used for other applications than performance measurement. "
      ]
    }
  ]
}